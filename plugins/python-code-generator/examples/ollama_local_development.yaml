# Esempio Ollama: Generazione Codice con LLM Locale
# Dimostra l'utilizzo del plugin con Ollama per sviluppo offline

variables:
  ollama_url: "http://localhost:11434"  # URL del server Ollama locale
  model_name: "codellama"  # Modello Ollama per code generation
  project_name: "web_scraper"
  target_websites: "e-commerce, news, social media"

start_state: "check_ollama"

states:
  check_ollama:
    state_type: "command"
    action:
      eval: |
        import requests
        try:
            response = requests.get("{ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                print(f"✅ Ollama connesso! Modelli disponibili: {', '.join(model_names)}")
                if "{model_name}" in model_names:
                    print(f"✅ Modello {model_name} trovato")
                else:
                    print(f"⚠️ Modello {model_name} non trovato. Modelli disponibili: {model_names}")
            else:
                print(f"❌ Errore connessione Ollama: {response.status_code}")
        except Exception as e:
            print(f"❌ Ollama non raggiungibile: {e}")
            print("💡 Assicurati che Ollama sia avviato: ollama serve")
    transition: "generate_scraper"

  generate_scraper:
    state_type: "python_code_generator"
    prompt: |
      Crea un sistema di web scraping modulare per {target_websites} con le seguenti caratteristiche:
      
      FUNZIONALITÀ CORE:
      - Scraping multi-sito con configurazione JSON
      - Gestione rate limiting e rispetto robots.txt
      - Supporto JavaScript con Selenium (opzionale)
      - Parsing intelligente con BeautifulSoup e lxml
      - Estrazione dati strutturati (JSON, CSV, Database)
      - Gestione errori robusta e retry logic
      - Logging dettagliato e monitoring
      - Cache per evitare richieste duplicate
      
      ARCHITETTURA:
      - Pattern Strategy per diversi tipi di siti
      - Queue system per gestire richieste
      - Plugin system per parser personalizzati
      - Configuration management
      - Data pipeline con validazione
      
      REQUISITI TECNICI:
      - Python 3.8+ compatibile
      - Async/await per performance
      - Type hints completi
      - Documentazione dettagliata
      - Test coverage > 80%
    max_steps: 25
    provider: "ollama"
    model: "{model_name}"
    base_url: "{ollama_url}"
    temperature: 0.4
    complexity_level: "complex"
    code_style: "pep8"
    include_tests: true
    include_documentation: true
    validate_syntax: true
    execution_mode: "full"
    output: "scraper_result"
    code_output: "scraper_code"
    tests_output: "scraper_tests"
    plan_output: "scraper_plan"
    success_transition: "analyze_generation"
    error_transition: "handle_error"

  analyze_generation:
    state_type: "command"
    action:
      eval: |
        print("🎉 WEB SCRAPER GENERATO CON OLLAMA!")
        print("="*60)
        print(f"🤖 Provider: {scraper_result.provider} (locale)")
        print(f"🧠 Modello: {scraper_result.model}")
        print(f"📊 Step eseguiti: {scraper_result.steps_executed}")
        print(f"📈 Step rimanenti: {scraper_result.steps_remaining}")
        print(f"⏱️ Tempo totale: {scraper_result.execution_time:.2f}s")
        print(f"🔄 Revisioni piano: {scraper_result.plan_revisions}")
        
        # Analisi performance
        avg_time = scraper_result.performance_metrics.average_step_time
        print(f"⚡ Tempo medio per step: {avg_time:.2f}s")
        
        # Analisi codice generato
        code_lines = len(scraper_code.split('\n'))
        test_lines = len(scraper_tests.split('\n'))
        print(f"📝 Linee di codice: {code_lines}")
        print(f"🧪 Linee di test: {test_lines}")
        print(f"📋 Fasi del piano: {len(scraper_result.execution_plan)}")
        
        print("\n📋 PIANO ESEGUITO:")
        print("-" * 60)
        for i, step in enumerate(scraper_result.execution_plan, 1):
            print(f"{i:2d}. {step['title']}")
            if step.get('components'):
                components = ", ".join(step['components'][:3])  # Prime 3
                if len(step['components']) > 3:
                    components += f" (+{len(step['components'])-3} altri)"
                print(f"     🔧 {components}")
    transition: "create_project_structure"

  create_project_structure:
    state_type: "command"
    action:
      eval: |
        import os
        
        # Crea struttura directory del progetto
        project_dirs = [
            f"{project_name}",
            f"{project_name}/src",
            f"{project_name}/src/scrapers",
            f"{project_name}/src/parsers", 
            f"{project_name}/src/utils",
            f"{project_name}/tests",
            f"{project_name}/config",
            f"{project_name}/data",
            f"{project_name}/logs",
            f"{project_name}/docs"
        ]
        
        for dir_path in project_dirs:
            os.makedirs(dir_path, exist_ok=True)
            print(f"📁 Creata directory: {dir_path}")
        
        print(f"\n✅ Struttura progetto {project_name} creata!")
    transition: "save_main_code"

  save_main_code:
    state_type: "file"
    action: "write"
    path: "{project_name}/src/main.py"
    content: |
      """
      {project_name.title()} - Sistema di Web Scraping
      
      Generato automaticamente da IntellyHub Python Code Generator
      Provider: {scraper_result.provider} ({scraper_result.model})
      Generato il: {scraper_result.timestamp}
      Step eseguiti: {scraper_result.steps_executed}
      Tempo di generazione: {scraper_result.execution_time:.2f}s
      """
      
      {scraper_code}
    transition: "save_tests"

  save_tests:
    state_type: "file"
    action: "write"
    path: "{project_name}/tests/test_scraper.py"
    content: |
      """
      Test Suite per {project_name.title()}
      
      Generato automaticamente da IntellyHub Python Code Generator
      """
      
      {scraper_tests}
    transition: "save_config"

  save_config:
    state_type: "file"
    action: "write"
    path: "{project_name}/config/scraper_config.json"
    content: |
      {
        "project_info": {
          "name": "{project_name}",
          "version": "1.0.0",
          "generated_by": "IntellyHub Python Code Generator",
          "generation_date": "{scraper_result.timestamp}",
          "provider": "{scraper_result.provider}",
          "model": "{scraper_result.model}",
          "target_websites": "{target_websites}"
        },
        "scraping_config": {
          "rate_limit": {
            "requests_per_second": 1,
            "burst_limit": 5,
            "respect_robots_txt": true
          },
          "retry_config": {
            "max_retries": 3,
            "backoff_factor": 2,
            "timeout": 30
          },
          "output_formats": ["json", "csv"],
          "cache_enabled": true,
          "cache_ttl": 3600
        },
        "sites": {
          "example_ecommerce": {
            "base_url": "https://example-shop.com",
            "selectors": {
              "product_name": ".product-title",
              "price": ".price",
              "description": ".product-description"
            },
            "rate_limit": 0.5
          },
          "example_news": {
            "base_url": "https://example-news.com",
            "selectors": {
              "title": "h1.article-title",
              "content": ".article-content",
              "date": ".publish-date"
            },
            "rate_limit": 1.0
          }
        }
      }
    transition: "save_requirements"

  save_requirements:
    state_type: "file"
    action: "write"
    path: "{project_name}/requirements.txt"
    content: |
      # Core dependencies
      requests>=2.28.0
      beautifulsoup4>=4.11.0
      lxml>=4.9.0
      aiohttp>=3.8.0
      
      # Optional dependencies
      selenium>=4.8.0
      pandas>=1.5.0
      
      # Development dependencies
      pytest>=7.0.0
      pytest-asyncio>=0.21.0
      pytest-cov>=4.0.0
      black>=23.0.0
      flake8>=6.0.0
      
      # Monitoring
      prometheus-client>=0.16.0
    transition: "save_readme"

  save_readme:
    state_type: "file"
    action: "write"
    path: "{project_name}/README.md"
    content: |
      # 🕷️ {project_name.title()} - Sistema di Web Scraping
      
      Sistema di web scraping modulare generato automaticamente da **IntellyHub Python Code Generator**.
      
      ## 🤖 Informazioni Generazione
      
      - **Provider**: {scraper_result.provider} (Ollama locale)
      - **Modello**: {scraper_result.model}
      - **Generato il**: {scraper_result.timestamp}
      - **Step eseguiti**: {scraper_result.steps_executed}
      - **Tempo generazione**: {scraper_result.execution_time:.2f} secondi
      - **Revisioni piano**: {scraper_result.plan_revisions}
      
      ## 🎯 Caratteristiche
      
      - ✅ Scraping multi-sito configurabile
      - ✅ Rate limiting e rispetto robots.txt
      - ✅ Gestione errori e retry logic
      - ✅ Supporto async/await
      - ✅ Cache intelligente
      - ✅ Output multipli (JSON, CSV)
      - ✅ Logging dettagliato
      - ✅ Test coverage completa
      
      ## 🚀 Quick Start
      
      ### 1. Installazione
      
      ```bash
      cd {project_name}
      pip install -r requirements.txt
      ```
      
      ### 2. Configurazione
      
      Modifica `config/scraper_config.json` per i tuoi siti target.
      
      ### 3. Esecuzione
      
      ```bash
      python src/main.py
      ```
      
      ## 📁 Struttura Progetto
      
      ```
      {project_name}/
      ├── src/
      │   ├── main.py              # Entry point principale
      │   ├── scrapers/            # Moduli scraping
      │   ├── parsers/             # Parser per diversi siti
      │   └── utils/               # Utilities
      ├── tests/
      │   └── test_scraper.py      # Test suite
      ├── config/
      │   └── scraper_config.json  # Configurazione
      ├── data/                    # Output dati
      ├── logs/                    # File di log
      └── docs/                    # Documentazione
      ```
      
      ## 🧪 Testing
      
      ```bash
      # Esegui tutti i test
      pytest tests/ -v
      
      # Test con coverage
      pytest tests/ --cov=src --cov-report=html
      
      # Test specifici
      pytest tests/test_scraper.py::TestScraper::test_basic_scraping
      ```
      
      ## ⚙️ Configurazione Avanzata
      
      ### Rate Limiting
      
      ```json
      {
        "rate_limit": {
          "requests_per_second": 1,
          "burst_limit": 5,
          "respect_robots_txt": true
        }
      }
      ```
      
      ### Retry Logic
      
      ```json
      {
        "retry_config": {
          "max_retries": 3,
          "backoff_factor": 2,
          "timeout": 30
        }
      }
      ```
      
      ## 📊 Monitoring
      
      Il sistema include metriche Prometheus per monitoraggio:
      
      - Richieste totali
      - Errori per tipo
      - Tempo di risposta
      - Cache hit rate
      
      ## 🔧 Sviluppo
      
      ### Aggiungere un nuovo sito
      
      1. Aggiungi configurazione in `config/scraper_config.json`
      2. Crea parser specifico in `src/parsers/`
      3. Aggiungi test in `tests/`
      
      ### Code Quality
      
      ```bash
      # Formattazione
      black src/ tests/
      
      # Linting
      flake8 src/ tests/
      ```
      
      ## 📈 Piano di Sviluppo Utilizzato
      
      {% for step in scraper_result.execution_plan %}
      ### Step {{ loop.index }}: {{ step.title }}
      
      **Descrizione**: {{ step.description }}
      
      {% if step.components %}
      **Componenti implementati**:
      {% for component in step.components %}
      - {{ component }}
      {% endfor %}
      {% endif %}
      
      ---
      {% endfor %}
      
      ## 🤝 Supporto
      
      Per supporto sull'utilizzo di IntellyHub Python Code Generator:
      
      - 📧 Email: support@intellyhub.com
      - 🐛 Issues: [GitHub Issues](https://github.com/kuduk/intellyhub-plugins/issues)
      
      ## 📄 Licenza
      
      MIT License - Vedi LICENSE per dettagli.
      
      ---
      
      **Generato con ❤️ da IntellyHub Python Code Generator v1.0.0 + Ollama {model_name}**
    transition: "save_development_plan"

  save_development_plan:
    state_type: "file"
    action: "write"
    path: "{project_name}/docs/development_plan.json"
    content: |
      {
        "generation_info": {
          "project_name": "{project_name}",
          "target_websites": "{target_websites}",
          "generated_by": "IntellyHub Python Code Generator",
          "provider": "{scraper_result.provider}",
          "model": "{scraper_result.model}",
          "generation_date": "{scraper_result.timestamp}",
          "execution_time": "{scraper_result.execution_time:.2f}s",
          "steps_executed": "{scraper_result.steps_executed}",
          "plan_revisions": "{scraper_result.plan_revisions}"
        },
        "execution_plan": {scraper_plan},
        "performance_metrics": {
          "total_steps": "{scraper_result.performance_metrics.total_steps}",
          "average_step_time": "{scraper_result.performance_metrics.average_step_time:.2f}s",
          "step_timeline": {scraper_result.performance_metrics.step_times}
        }
      }
    transition: "create_docker_setup"

  create_docker_setup:
    state_type: "file"
    action: "write"
    path: "{project_name}/Dockerfile"
    content: |
      # Dockerfile per {project_name.title()}
      # Generato da IntellyHub Python Code Generator
      
      FROM python:3.11-slim
      
      WORKDIR /app
      
      # Installa dipendenze sistema
      RUN apt-get update && apt-get install -y \
          wget \
          gnupg \
          unzip \
          curl \
          && rm -rf /var/lib/apt/lists/*
      
      # Copia requirements e installa dipendenze Python
      COPY requirements.txt .
      RUN pip install --no-cache-dir -r requirements.txt
      
      # Copia codice applicazione
      COPY src/ ./src/
      COPY config/ ./config/
      
      # Crea directory per dati e logs
      RUN mkdir -p data logs
      
      # Espone porta per monitoring (se necessario)
      EXPOSE 8080
      
      # Comando di default
      CMD ["python", "src/main.py"]
    transition: "create_docker_compose"

  create_docker_compose:
    state_type: "file"
    action: "write"
    path: "{project_name}/docker-compose.yml"
    content: |
      # Docker Compose per {project_name.title()}
      # Generato da IntellyHub Python Code Generator
      
      version: '3.8'
      
      services:
        scraper:
          build: .
          container_name: {project_name}_scraper
          volumes:
            - ./data:/app/data
            - ./logs:/app/logs
            - ./config:/app/config
          environment:
            - PYTHONPATH=/app
            - LOG_LEVEL=INFO
          restart: unless-stopped
          networks:
            - scraper_network
        
        redis:
          image: redis:7-alpine
          container_name: {project_name}_redis
          ports:
            - "6379:6379"
          volumes:
            - redis_data:/data
          networks:
            - scraper_network
        
        prometheus:
          image: prom/prometheus:latest
          container_name: {project_name}_prometheus
          ports:
            - "9090:9090"
          volumes:
            - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
          networks:
            - scraper_network
      
      volumes:
        redis_data:
      
      networks:
        scraper_network:
          driver: bridge
    transition: "finalize_project"

  finalize_project:
    state_type: "command"
    action:
      eval: |
        print("\n🎉 PROGETTO WEB SCRAPER COMPLETATO!")
        print("="*60)
        print(f"📁 Progetto creato: {project_name}/")
        print(f"🤖 Generato con Ollama {model_name} (locale)")
        print(f"⏱️ Tempo totale: {scraper_result.execution_time:.2f}s")
        print(f"📊 Step utilizzati: {scraper_result.steps_executed}")
        
        print("\n📁 File creati:")
        files_created = [
            "src/main.py",
            "tests/test_scraper.py", 
            "config/scraper_config.json",
            "requirements.txt",
            "README.md",
            "docs/development_plan.json",
            "Dockerfile",
            "docker-compose.yml"
        ]
        
        for file in files_created:
            print(f"   ✅ {project_name}/{file}")
        
        print(f"\n📊 Statistiche codice:")
        code_lines = len(scraper_code.split('\n'))
        test_lines = len(scraper_tests.split('\n'))
        print(f"   📝 Linee di codice: {code_lines}")
        print(f"   🧪 Linee di test: {test_lines}")
        print(f"   📋 Fasi pianificate: {len(scraper_result.execution_plan)}")
        
        print("\n🚀 Prossimi passi:")
        print(f"   1. cd {project_name}")
        print("   2. pip install -r requirements.txt")
        print("   3. Modifica config/scraper_config.json")
        print("   4. python src/main.py")
        print("   5. pytest tests/ -v")
        
        print("\n🐳 Deployment Docker:")
        print("   docker-compose up -d")
        
        print("\n✨ Buon scraping!")
    transition: "end"

  handle_error:
    state_type: "command"
    action:
      eval: |
        print("❌ ERRORE NELLA GENERAZIONE")
        print("="*50)
        
        if 'scraper_result' in locals():
            print(f"Errore: {scraper_result.get('error', 'Errore sconosciuto')}")
            print(f"Step completati: {scraper_result.get('steps_executed', 0)}")
            
            if scraper_result.get('partial_code'):
                print("\n🔧 Codice parziale disponibile")
                print("Salvando il codice parziale...")
                
                # Salva codice parziale
                import os
                os.makedirs(f"{project_name}/src", exist_ok=True)
                with open(f"{project_name}/src/partial_main.py", "w") as f:
                    f.write(f"# Codice parziale generato\n{scraper_result.partial_code}")
                print(f"💾 Salvato in {project_name}/src/partial_main.py")
        else:
            print("Errore durante l'inizializzazione")
        
        print("\n💡 Suggerimenti per Ollama:")
        print("   • Verifica che Ollama sia avviato: ollama serve")
        print(f"   • Controlla che il modello {model_name} sia installato")
        print(f"   • Verifica connessione: curl {ollama_url}/api/tags")
        print("   • Riduci max_steps se il modello è lento")
        print("   • Semplifica il prompt per modelli più piccoli")
    transition: "end"

  end:
    state_type: "end"
